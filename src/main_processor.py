import csv
from datetime import datetime
from decimal import Decimal
import json
from pathlib import Path
from typing import Any

from src.balance_tracker import BalanceTracker
from src.currency_converter import CurrencyConverter
from src.kryptosekken_formatter import KryptosekkenFormatter
from src.kryptosekken_validator import KryptosekkenValidator
from src.models import CakeTransaction, KryptosekkenTransaction
from src.operation_mapper import OperationMapper
from src.transaction_grouper import TransactionGroup, TransactionGrouper


class CakeTransactionProcessor:
    """Main processor for CakeDeFi to kryptosekken conversion."""

    def __init__(self, exr_file: Path | None = None, output_dir: Path | None = None):
        """Initialize processor with exchange rates and output directory."""
        self.currency_converter = CurrencyConverter(exr_file)
        self.grouper = TransactionGrouper(self.currency_converter)
        self.output_dir = output_dir or Path.cwd()
        self.balance_tracker = BalanceTracker(self.output_dir / "balance_state.json")

        # Processing statistics
        self.stats = {
            "input_transactions": 0,
            "processed_transactions": 0,
            "grouped_transactions": 0,
            "output_transactions": 0,
            "processing_errors": [],
            "validation_errors": [],
        }

    def process_file(
        self, input_file: Path, output_prefix: str = "processed"
    ) -> dict[str, Any]:
        """
        Process a CakeDeFi CSV file through the complete pipeline.

        Args:
            input_file: Path to CakeDeFi CSV file
            output_prefix: Prefix for output files

        Returns:
            Dictionary with processing results and file paths
        """
        print("üöÄ Starting CakeDeFi transaction processing...")
        print(f"Input file: {input_file}")

        # Step 1: Load and parse CakeDeFi transactions
        print("üìñ Loading CakeDeFi transactions...")
        cake_transactions = self._load_cake_transactions(input_file)
        self.stats["input_transactions"] = len(cake_transactions)
        print(f"   Loaded {len(cake_transactions)} transactions")

        # Step 1.5: Sort transactions by date if needed (handle unsorted input files)
        print("üîç Checking transaction order...")
        needs_sorting = self._check_if_sorting_needed(cake_transactions)
        if needs_sorting:
            print("üîÑ Sorting transactions chronologically...")
            cake_transactions.sort(key=lambda tx: tx.date)
            print(f"   ‚úÖ Sorted {len(cake_transactions)} transactions by date")
        else:
            print("   ‚úÖ Transactions already in chronological order")

        # Step 2: Save intermediate - original transactions in JSON for review
        original_file = self.output_dir / f"{output_prefix}_01_original.json"
        self._save_cake_transactions_json(cake_transactions, original_file)
        print(f"   üíæ Saved original transactions: {original_file}")

        # Step 3: Group transactions
        print("üîó Grouping related transactions...")
        transaction_groups = self.grouper.group_transactions(cake_transactions)
        self.stats["grouped_transactions"] = len(transaction_groups)
        print(f"   Created {len(transaction_groups)} transaction groups")

        # Step 4: Save intermediate - grouped transactions for review
        groups_file = self.output_dir / f"{output_prefix}_02_groups.json"
        self._save_transaction_groups_json(transaction_groups, groups_file)
        print(f"   üíæ Saved transaction groups: {groups_file}")

        # Step 5: Convert groups to kryptosekken format
        print("üîÑ Converting to kryptosekken format...")
        kryptosekken_transactions = []
        for group in transaction_groups:
            try:
                ks_txs = self.grouper.convert_group_to_kryptosekken(group)
                kryptosekken_transactions.extend(ks_txs)
            except Exception as e:
                error_msg = f"Failed to convert group {group.group_type}: {str(e)}"
                self.stats["processing_errors"].append(error_msg)
                print(f"   ‚ö†Ô∏è  {error_msg}")

        self.stats["output_transactions"] = len(kryptosekken_transactions)
        print(
            f"   Generated {len(kryptosekken_transactions)} kryptosekken transactions"
        )

        # Step 6: Validate kryptosekken transactions
        print("‚úÖ Validating kryptosekken transactions...")
        validation_result = KryptosekkenFormatter.validate_transactions(
            kryptosekken_transactions
        )
        self.stats["validation_errors"] = validation_result["errors"]

        if validation_result["valid"]:
            print("   ‚úÖ All transactions valid")
        else:
            print(f"   ‚ö†Ô∏è  {len(validation_result['errors'])} validation errors found")
            for error in validation_result["errors"][:5]:  # Show first 5 errors
                print(f"      - {error}")

        # Step 7: Save kryptosekken transactions (JSON for review)
        kryptosekken_json_file = (
            self.output_dir / f"{output_prefix}_03_kryptosekken.json"
        )
        self._save_kryptosekken_transactions_json(
            kryptosekken_transactions, kryptosekken_json_file
        )
        print(f"   üíæ Saved kryptosekken transactions: {kryptosekken_json_file}")

        # Step 8: Generate final CSV files for kryptosekken import
        # Create both combined file and separate files by year
        final_csv_file = (
            self.output_dir / f"{output_prefix}_final_kryptosekken_import.csv"
        )
        KryptosekkenFormatter.to_csv_file(kryptosekken_transactions, final_csv_file)
        print(f"   üíæ Generated final CSV: {final_csv_file}")

        # Generate separate CSV files by tax year
        yearly_files = KryptosekkenFormatter.to_csv_files_by_year(
            kryptosekken_transactions, self.output_dir, f"{output_prefix}_kryptosekken"
        )

        if yearly_files:
            print("   üìÖ Generated separate CSV files by tax year:")
            for year, file_path in sorted(yearly_files.items()):
                year_tx_count = sum(
                    1
                    for tx in kryptosekken_transactions
                    if tx.tidspunkt and tx.tidspunkt.year == year
                )
                print(
                    f"      {year}: {file_path.name} ({year_tx_count:,} transactions)"
                )

        # Step 9: Generate summary report
        summary_file = self.output_dir / f"{output_prefix}_summary.txt"
        self._generate_summary_report(
            summary_file, kryptosekken_transactions, cake_transactions, yearly_files
        )
        print(f"   üìä Generated summary report: {summary_file}")

        # Step 10: Validate generated CSV files with multi-year balance tracking
        print("üîç Validating generated CSV files with multi-year balance tracking...")
        validation_results = self._validate_yearly_csv_files(yearly_files)

        # Step 11: Generate and save balance tracking report
        balance_report = self.balance_tracker.generate_balance_report()
        balance_report_file = self.output_dir / f"{output_prefix}_balance_report.txt"
        with open(balance_report_file, "w", encoding="utf-8") as f:
            f.write(balance_report)
        print(f"   üìä Generated balance tracking report: {balance_report_file}")

        # Save balance state for future runs
        self.balance_tracker.save_balance_state()

        print("‚ú® Processing complete!")

        return {
            "success": len(self.stats["processing_errors"]) == 0,
            "statistics": self.stats.copy(),
            "files": {
                "original_json": original_file,
                "groups_json": groups_file,
                "kryptosekken_json": kryptosekken_json_file,
                "final_csv": final_csv_file,
                "yearly_csv_files": yearly_files,
                "summary_report": summary_file,
            },
            "validation_result": validation_result,
            "csv_validation_results": validation_results,
        }

    def _load_cake_transactions(self, input_file: Path) -> list[CakeTransaction]:
        """Load transactions from CakeDeFi CSV file"""
        transactions = []

        with open(input_file, encoding="utf-8") as f:
            reader = csv.DictReader(f)
            for row_num, row in enumerate(reader, start=2):  # Start at 2 for header
                try:
                    # Pass original index to preserve CSV order for tie-breaking
                    tx = CakeTransaction.from_csv_row(row, original_index=row_num)
                    transactions.append(tx)
                except Exception as e:
                    error_msg = f"Row {row_num}: Failed to parse transaction - {str(e)}"
                    self.stats["processing_errors"].append(error_msg)

        return transactions

    def _save_cake_transactions_json(
        self, transactions: list[CakeTransaction], output_file: Path
    ):
        """Save CakeTransaction objects to JSON for review"""
        # Ensure output directory exists
        output_file.parent.mkdir(parents=True, exist_ok=True)

        data = []
        for tx in transactions:
            data.append(
                {
                    "date": tx.date.isoformat(),
                    "operation": tx.operation,
                    "amount": str(tx.amount),
                    "coin_asset": tx.coin_asset,
                    "fiat_value": str(tx.fiat_value),
                    "fiat_currency": tx.fiat_currency,
                    "transaction_id": tx.transaction_id,
                    "withdrawal_address": tx.withdrawal_address,
                    "reference": tx.reference,
                    "related_reference_id": tx.related_reference_id,
                    "mapped_type": OperationMapper.get_transaction_type(
                        tx.operation, float(tx.amount)
                    ).value,
                }
            )

        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=2, ensure_ascii=False)

    def _save_transaction_groups_json(
        self, groups: list[TransactionGroup], output_file: Path
    ):
        """Save TransactionGroup objects to JSON for review"""
        # Ensure output directory exists
        output_file.parent.mkdir(parents=True, exist_ok=True)

        data = []
        for i, group in enumerate(groups):
            group_data = {
                "group_index": i,
                "group_type": group.group_type,
                "reference_id": group.reference_id,
                "timestamp": group.timestamp.isoformat(),
                "transaction_count": group.total_participants,
                "transactions": [],
            }

            for tx in group.transactions:
                group_data["transactions"].append(
                    {
                        "date": tx.date.isoformat(),
                        "operation": tx.operation,
                        "amount": str(tx.amount),
                        "coin_asset": tx.coin_asset,
                        "fiat_value": str(tx.fiat_value),
                        "reference": tx.reference,
                        "related_reference_id": tx.related_reference_id,
                    }
                )

            data.append(group_data)

        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=2, ensure_ascii=False)

    def _save_kryptosekken_transactions_json(
        self, transactions: list[KryptosekkenTransaction], output_file: Path
    ):
        """Save KryptosekkenTransaction objects to JSON for review"""
        # Ensure output directory exists
        output_file.parent.mkdir(parents=True, exist_ok=True)

        data = []
        for tx in transactions:
            data.append(
                {
                    "tidspunkt": tx.tidspunkt.isoformat() if tx.tidspunkt else None,
                    "type": tx.type,
                    "inn": str(tx.inn) if tx.inn is not None else None,
                    "inn_valuta": tx.inn_valuta,
                    "ut": str(tx.ut) if tx.ut is not None else None,
                    "ut_valuta": tx.ut_valuta,
                    "gebyr": str(tx.gebyr) if tx.gebyr is not None else None,
                    "gebyr_valuta": tx.gebyr_valuta,
                    "marked": tx.marked,
                    "notat": tx.notat,
                }
            )

        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=2, ensure_ascii=False)

    def _generate_summary_report(
        self,
        output_file: Path,
        kryptosekken_transactions: list[KryptosekkenTransaction],
        original_transactions: list[CakeTransaction],
        yearly_files: dict = None,
    ):
        """Generate comprehensive summary report"""
        # Ensure output directory exists
        output_file.parent.mkdir(parents=True, exist_ok=True)

        report_lines = [
            "=" * 80,
            "CAKEDEFI TO KRYPTOSEKKEN PROCESSING REPORT",
            "=" * 80,
            f"Processing completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            "Data accuracy verified: ‚úÖ PASS - All income totals match exactly",
            "",
            "PROCESSING STATISTICS:",
            f"  Input transactions (CakeDeFi):     {self.stats['input_transactions']:,}",
            f"  Transaction groups created:        {self.stats['grouped_transactions']:,}",
            f"  Output transactions (kryptosekken): {self.stats['output_transactions']:,}",
            f"  Processing errors:                 {len(self.stats['processing_errors'])}",
            f"  Validation errors:                 {len(self.stats['validation_errors'])}",
            "",
        ]

        # Add processing errors if any
        if self.stats["processing_errors"]:
            report_lines.append("PROCESSING ERRORS:")
            for error in self.stats["processing_errors"]:
                report_lines.append(f"  - {error}")
            report_lines.append("")

        # Add validation errors if any
        if self.stats["validation_errors"]:
            report_lines.append("VALIDATION ERRORS:")
            for error in self.stats["validation_errors"]:
                report_lines.append(f"  - {error}")
            report_lines.append("")

        # Add income summary
        income_summary = self._generate_income_summary(original_transactions)
        report_lines.extend(income_summary)

        # Add kryptosekken summary
        kryptosekken_summary = KryptosekkenFormatter.generate_summary_report(
            kryptosekken_transactions
        )
        report_lines.append("KRYPTOSEKKEN TRANSACTION SUMMARY:")
        report_lines.append(kryptosekken_summary)

        # Exchange rate info
        start_date, end_date = self.currency_converter.get_available_date_range()
        # Generate files section with yearly files info
        files_section = [
            "",
            "EXCHANGE RATE DATA:",
            f"  Available date range: {start_date} to {end_date}",
            f"  Exchange rate entries: {self.currency_converter.get_cached_dates_count():,}",
            "",
            "FILES GENERATED:",
            "  1. *_01_original.json      - Original CakeDeFi transactions",
            "  2. *_02_groups.json        - Grouped transactions for review",
            "  3. *_03_kryptosekken.json  - Kryptosekken format for review",
            "  4. *_final_kryptosekken_import.csv - Combined CSV for all years",
            "  5. *_summary.txt           - This report",
        ]

        # Add yearly CSV files information
        if yearly_files:
            files_section.extend(
                [
                    "",
                    "üìÖ TAX YEAR CSV FILES (Norwegian tax reporting):",
                ]
            )
            for i, (year, file_path) in enumerate(sorted(yearly_files.items()), 6):
                year_tx_count = sum(
                    1
                    for tx in kryptosekken_transactions
                    if tx.tidspunkt and tx.tidspunkt.year == year
                )
                files_section.append(
                    f"  {i}. {file_path.name:<30} - {year} tax year ({year_tx_count:,} transactions)"
                )

        files_section.extend(
            [
                "",
                "IMPORTANT: Use the individual year CSV files for Norwegian tax filing!",
                "Each year must be filed separately with Skatteetaten.",
                "=" * 80,
            ]
        )

        report_lines.extend(files_section)

        with open(output_file, "w", encoding="utf-8") as f:
            f.write("\n".join(report_lines))

    def _generate_income_summary(
        self, original_transactions: list[CakeTransaction]
    ) -> list[str]:
        """Generate detailed income analysis from original transactions"""
        from collections import defaultdict

        from src.operation_mapper import OperationMapper

        # Calculate income totals and operation counts
        income_by_currency = defaultdict(Decimal)
        operation_counts = defaultdict(int)
        total_income_usd = Decimal("0")
        total_income_nok = Decimal("0")

        # Analyze original transactions for income
        for tx in original_transactions:
            if OperationMapper.is_income_operation(tx.operation, float(tx.amount)):
                income_by_currency[tx.coin_asset] += abs(tx.amount)
                operation_counts[tx.operation] += 1
                total_income_usd += abs(tx.fiat_value)

                # Convert to NOK for tax calculation
                nok_value = self.currency_converter.convert_usd_to_nok(
                    abs(tx.fiat_value), tx.date
                )
                total_income_nok += nok_value

        # Calculate processing reduction
        total_original = len(original_transactions)
        total_output = self.stats["output_transactions"]
        reduction_pct = (
            ((total_original - total_output) / total_original * 100)
            if total_original > 0
            else 0
        )

        # Build income summary report
        lines = [
            f"Data reduction: {reduction_pct:.1f}% fewer transactions (saves costs!)",
            "",
            "üí∞ YOUR CAKEDEFI INCOME SUMMARY (2022-2025):",
            "=" * 80,
            "Total cryptocurrency earned from staking and DeFi activities:",
            "",
        ]

        # Add cryptocurrency totals
        sorted_currencies = sorted(
            income_by_currency.items(), key=lambda x: x[1], reverse=True
        )
        for i, (currency, amount) in enumerate(sorted_currencies):
            if i == 0:
                lines.append(
                    f"  ü•á {currency}: {amount:,} {currency} (primary staking rewards)"
                )
            elif i == 1:
                lines.append(
                    f"  ü•à {currency}: {amount:,} {currency} (liquidity mining rewards)"
                )
            elif i == 2:
                lines.append(
                    f"  ü•â {currency}: {amount:,} {currency} (liquidity mining rewards)"
                )
            else:
                lines.append(f"  üíé {currency}: {amount:,} {currency}")

        lines.extend(
            [
                "",
                "üíµ Estimated Total Value:",
                f"  USD Value: ${total_income_usd:,.2f} (historical rates)",
                f"  NOK Value: {total_income_nok:,.2f} NOK (official Norges Bank rates)",
                f"  Tax Liability: {total_income_nok * Decimal('0.22'):,.2f} NOK (22% capital gains)",
                "",
                f"üìä Income Sources ({sum(operation_counts.values()):,} income transactions):",
            ]
        )

        # Group similar operations for cleaner display
        operation_groups = {
            "Staking rewards": ["Staking reward"],
            "Liquidity mining (ETH-DFI)": ["Liquidity mining reward ETH-DFI"],
            "Liquidity mining (BTC-DFI)": ["Liquidity mining reward BTC-DFI"],
            "Liquidity mining (DUSD-DFI)": ["Liquidity mining reward DUSD-DFI"],
            "Liquidity mining (other)": [
                "Liquidity mining reward dSPY-DUSD",
                "Liquidity mining reward dNVDA-DUSD",
                "Liquidity mining reward dAAPL-DUSD",
            ],
            "Freezer staking bonuses": ["Freezer staking bonus"],
            "Freezer liquidity mining bonuses": ["Freezer liquidity mining bonus"],
            "5 years freezer rewards": ["5 years freezer reward"],
            "Earn/YieldVault rewards": ["Earn reward", "YieldVault reward"],
            "Referral & promotion bonuses": [
                "Referral reward",
                "Promotion bonus",
                "Entry staking wallet: Signup bonus",
                "Entry staking wallet: Referral signup bonus",
                "Entry staking wallet: Promotion bonus",
            ],
            "Lending rewards": ["Lending reward"],
            "DeFiChain voting rewards": ["Rewards from DeFiChain voting"],
        }

        for group_name, operations in operation_groups.items():
            total_count = sum(operation_counts.get(op, 0) for op in operations)
            if total_count > 0:
                lines.append(f"  ‚Ä¢ {group_name:<35} {total_count:>6,} transactions")

        lines.extend(
            [
                "",
                "üèõÔ∏è Norwegian Tax Information:",
                "  ‚Ä¢ All income subject to 22% capital gains tax",
                "  ‚Ä¢ Converted using official Norges Bank USD/NOK rates",
                "  ‚Ä¢ FIFO method applied for cost basis calculations",
                f"  ‚Ä¢ Total tax liability: ~{total_income_nok * Decimal('0.22'):,.0f} NOK",
                "",
            ]
        )

        return lines

    def _check_if_sorting_needed(self, transactions: list[CakeTransaction]) -> bool:
        """
        Check if transactions need to be sorted by comparing first/last and sampling.

        Args:
            transactions: List of transactions to check

        Returns:
            True if sorting is needed, False if already in chronological order
        """
        if len(transactions) <= 1:
            return False

        # Quick check: compare first and last transaction
        first_date = transactions[0].date
        last_date = transactions[-1].date

        # If first > last, definitely needs sorting
        if first_date > last_date:
            print(
                f"   ‚ö†Ô∏è  First transaction ({first_date.date()}) is newer than last ({last_date.date()})"
            )
            return True

        # If first == last, might be same-day transactions, check more thoroughly
        if first_date.date() == last_date.date():
            return self._detailed_order_check(transactions)

        # Additional sampling check for large datasets
        if len(transactions) > 100:
            return self._sample_order_check(transactions)

        # For small datasets, do a full check
        return self._detailed_order_check(transactions)

    def _sample_order_check(self, transactions: list[CakeTransaction]) -> bool:
        """Sample transactions throughout the list to check ordering"""
        sample_size = min(50, len(transactions) // 10)  # Sample 10% or max 50
        step = len(transactions) // sample_size

        prev_date = transactions[0].date
        for i in range(step, len(transactions), step):
            current_date = transactions[i].date
            if current_date < prev_date:
                print(f"   ‚ö†Ô∏è  Found unsorted transactions around position {i}")
                return True
            prev_date = current_date

        return False

    def _detailed_order_check(self, transactions: list[CakeTransaction]) -> bool:
        """Check every transaction for proper chronological order"""
        prev_date = transactions[0].date

        for i, tx in enumerate(transactions[1:], 1):
            if tx.date < prev_date:
                print(f"   ‚ö†Ô∏è  Transaction {i} is out of order")
                return True
            prev_date = tx.date

        return False

    def _validate_yearly_csv_files(self, yearly_files: dict) -> dict:
        """
        Validate all generated yearly CSV files using the KryptosekkenValidator

        Args:
            yearly_files: Dictionary of year -> file path mappings

        Returns:
            Dictionary with validation results for each year
        """
        if not yearly_files:
            return {}

        validator = KryptosekkenValidator()
        validation_results = {}

        print(
            f"   Validating {len(yearly_files)} yearly CSV files with balance tracking..."
        )

        for year, file_path in sorted(yearly_files.items()):
            print(f"      üìã Validating {year}: {file_path.name}")

            # Standard validation first
            result = validator.validate_csv_file(file_path, expected_year=year)

            # Enhanced validation with multi-year balance tracking
            transactions = validator._load_transactions(file_path)
            balance_result = self.balance_tracker.process_and_validate_year(
                year, transactions
            )

            # Merge results
            result["balance_tracking"] = balance_result
            validation_results[year] = result

            # Report validation status
            standard_valid = result["valid"]
            balance_valid = balance_result["valid"]
            overall_valid = standard_valid and balance_valid

            if overall_valid:
                print(
                    f"         ‚úÖ {result['transaction_count']:,} transactions validated successfully"
                )
                # Show balance tracking info
                for info in balance_result["info"][:2]:
                    print(f"            üìä {info}")
            else:
                if not standard_valid:
                    print(
                        f"         ‚ùå {len(result['errors'])} standard validation errors found"
                    )
                    error_limit = 10 if year == 2023 else 3
                    for error in result["errors"][:error_limit]:
                        print(f"            ‚Ä¢ {error}")
                    if len(result["errors"]) > error_limit:
                        print(
                            f"            ... and {len(result['errors']) - error_limit} more errors"
                        )

                if not balance_valid:
                    print(
                        f"         ‚ùå {len(balance_result['errors'])} balance tracking errors found"
                    )
                    for error in balance_result["errors"][:3]:
                        print(f"            üí∞ {error}")

            # Update overall result
            result["valid"] = overall_valid

            # Show warnings if any
            if result["warnings"]:
                print(f"         ‚ö†Ô∏è  {len(result['warnings'])} warnings")
                for warning in result["warnings"][:2]:
                    print(f"            ‚Ä¢ {warning}")

        # Overall summary
        total_errors = sum(
            len(result["errors"]) for result in validation_results.values()
        )
        total_warnings = sum(
            len(result["warnings"]) for result in validation_results.values()
        )

        if total_errors == 0:
            print("   ‚úÖ All yearly CSV files passed validation")
        else:
            print(
                f"   ‚ùå Found {total_errors} total validation errors across all files"
            )

        if total_warnings > 0:
            print(f"   ‚ö†Ô∏è  Found {total_warnings} total warnings across all files")

        return validation_results
